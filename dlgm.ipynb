{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1RsHxsPLr85Twg-Jb7hp4NDLY_tSZfeTj",
      "authorship_tag": "ABX9TyN7U0XQGYiwYVlhZ9gGTRT9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tahirabatool123/Task/blob/main/dlgm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0KTFXdDdQXNW",
        "outputId": "75fd7921-53fc-49fa-87f3-26ff316fd4e9"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "de90jea20_Lg"
      },
      "outputs": [],
      "source": [
        "!pip install torch torchvision --quiet\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# VAE for MNIST - PyTorch implementation\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms, utils\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "# Reproducibility\n",
        "seed = 42\n",
        "torch.manual_seed(seed)\n",
        "np.random.seed(seed)\n",
        "random.seed(seed)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# ------------------------\n",
        "# Model Class\n",
        "# ------------------------\n",
        "class VAEModel(nn.Module):\n",
        "    def __init__(self, latent_dim=20):\n",
        "        super(VAEModel, self).__init__()\n",
        "        self.latent_dim = latent_dim\n",
        "        # Encoder: Conv layers -> flatten -> linear -> mu/logvar\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(1, 32, 4, 2, 1),  # 28->14\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, 4, 2, 1), # 14->7\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 128, 3, 2, 1), # 7->4 (approx)\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "        # compute flattened size dynamically (we know approx 128*4*4=2048)\n",
        "        self.fc_mu = nn.Linear(128*4*4, latent_dim)\n",
        "        self.fc_logvar = nn.Linear(128*4*4, latent_dim)\n",
        "\n",
        "        # Decoder: linear -> reshape -> ConvTranspose layers\n",
        "        self.fc_dec = nn.Linear(latent_dim, 128*4*4)\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.ConvTranspose2d(128, 64, 4, 2, 1), # 4->8\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(64, 32, 4, 2, 1),  # 8->16\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(32, 16, 4, 2, 1),  # 16->32\n",
        "            nn.ReLU(),\n",
        "            # final conv to get back to 28x28 (we will crop/resize to 28)\n",
        "            nn.Conv2d(16, 1, 3, 1, 1),\n",
        "            nn.Sigmoid()  # output pixels in [0,1]\n",
        "        )\n",
        "\n",
        "    def encode(self, x):\n",
        "        h = self.encoder(x)                       # [B,128,4,4]\n",
        "        h = h.view(h.size(0), -1)                 # [B, 128*4*4]\n",
        "        mu = self.fc_mu(h)\n",
        "        logvar = self.fc_logvar(h)\n",
        "        return mu, logvar\n",
        "\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        # logvar is log(sigma^2)\n",
        "        std = torch.exp(0.5 * logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        z = mu + std * eps\n",
        "        return z\n",
        "\n",
        "    def decode(self, z):\n",
        "        h = self.fc_dec(z)\n",
        "        h = h.view(-1, 128, 4, 4)                 # reshape\n",
        "        x_rec = self.decoder(h)                   # may be 32x32 -> we'll center-crop to 28\n",
        "        # center-crop to 28x28 if needed\n",
        "        if x_rec.shape[-1] != 28:\n",
        "            # crop centered\n",
        "            _, _, H, W = x_rec.shape\n",
        "            start_h = (H - 28) // 2\n",
        "            start_w = (W - 28) // 2\n",
        "            x_rec = x_rec[:, :, start_h:start_h+28, start_w:start_w+28]\n",
        "        return x_rec\n",
        "\n",
        "    def forward(self, x):\n",
        "        mu, logvar = self.encode(x)\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "        recon = self.decode(z)\n",
        "        return recon, mu, logvar\n",
        "\n",
        "# ------------------------\n",
        "# Loss Function\n",
        "# ------------------------\n",
        "def vae_loss_function(reconstruction, mu, log_var, original):\n",
        "    \"\"\"\n",
        "    reconstruction, original: tensors in [0,1], shape [B,1,28,28]\n",
        "    mu, log_var: [B, latent_dim]\n",
        "    \"\"\"\n",
        "    # Reconstruction loss: BCE summed over pixels then mean over batch\n",
        "    bce = F.binary_cross_entropy(reconstruction, original, reduction='sum')  # sum over all pixels and batch\n",
        "    # KL divergence per batch summed\n",
        "    kl = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
        "    # For optimization we return total loss\n",
        "    total_loss = bce + kl\n",
        "    # To return per-image averages for tracking:\n",
        "    batch_size = original.size(0)\n",
        "    recon_loss_per_image = bce / batch_size\n",
        "    kl_per_image = kl / batch_size\n",
        "    return total_loss, recon_loss_per_image.item(), kl_per_image.item()\n",
        "\n",
        "# ------------------------\n",
        "# Training Function\n",
        "# ------------------------\n",
        "def train_vae(model, dataloader, epochs=30, lr=1e-3, print_every=1):\n",
        "    model = model.to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    model.train()\n",
        "    history = {'recon_per_image':[], 'kl_per_image':[]}\n",
        "    for epoch in range(1, epochs+1):\n",
        "        running_recon = 0.0\n",
        "        running_kl = 0.0\n",
        "        for batch_idx, (data, _) in enumerate(dataloader):\n",
        "            data = data.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            recon, mu, logvar = model(data)\n",
        "            loss, recon_per_image, kl_per_image = vae_loss_function(recon, mu, logvar, data)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_recon += recon_per_image\n",
        "            running_kl += kl_per_image\n",
        "        avg_recon = running_recon / len(dataloader)\n",
        "        avg_kl = running_kl / len(dataloader)\n",
        "        history['recon_per_image'].append(avg_recon)\n",
        "        history['kl_per_image'].append(avg_kl)\n",
        "        if epoch % print_every == 0:\n",
        "            print(f\"Epoch {epoch}/{epochs}  Recon_per_image: {avg_recon:.4f}  KL_per_image: {avg_kl:.4f}\")\n",
        "    return model, history\n",
        "\n",
        "# ------------------------\n",
        "# Sample Generation Function\n",
        "# ------------------------\n",
        "def generate_samples(model, num_samples=10):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        z = torch.randn(num_samples, model.latent_dim).to(device)\n",
        "        samples = model.decode(z)  # [num,1,28,28]\n",
        "        samples = samples.cpu()\n",
        "    # ensure shape (10,1,28,28)\n",
        "    return samples\n",
        "\n",
        "# ------------------------\n",
        "# Run Experiment\n",
        "# ------------------------\n",
        "def run_vae_experiment(batch_size=128, epochs=30, latent_dim=20, lr=1e-3, samples_to_generate=10):\n",
        "    # Data\n",
        "    transform = transforms.Compose([transforms.ToTensor()])\n",
        "    train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "    test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "    # Model\n",
        "    model = VAEModel(latent_dim=latent_dim).to(device)\n",
        "\n",
        "    # Train\n",
        "    model, history = train_vae(model, train_loader, epochs=epochs, lr=lr)\n",
        "\n",
        "    # Calculate final average KL on test set (per image)\n",
        "    model.eval()\n",
        "    total_kl = 0.0\n",
        "    batches = 0\n",
        "    originals_list = []\n",
        "    reconstructions_list = []\n",
        "    with torch.no_grad():\n",
        "        for data, _ in test_loader:\n",
        "            data = data.to(device)\n",
        "            recon, mu, logvar = model(data)\n",
        "            # compute kl per image for this batch\n",
        "            kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "            kl_per_image = kl / data.size(0)\n",
        "            total_kl += kl_per_image.item()\n",
        "            batches += 1\n",
        "            # store first batch images for reconstruction examples\n",
        "            if len(originals_list) < 5:\n",
        "                originals_list.append(data.cpu())\n",
        "                reconstructions_list.append(recon.cpu())\n",
        "    avg_kl_loss = total_kl / batches if batches > 0 else 0.0\n",
        "\n",
        "    # Reconstruction: take first 5 test images (flatten lists)\n",
        "    originals_tensor = torch.cat(originals_list, dim=0)[:5]      # shape (5,1,28,28)\n",
        "    recon_tensor = torch.cat(reconstructions_list, dim=0)[:5]    # shape (5,1,28,28)\n",
        "\n",
        "    # MSE for reconstructions (mean over pixels & images)\n",
        "    mse_value = torch.mean((originals_tensor - recon_tensor) ** 2).item()\n",
        "\n",
        "    # Generate samples\n",
        "    generated_samples = generate_samples(model, num_samples=samples_to_generate)  # (10,1,28,28)\n",
        "\n",
        "    # Pixel std across all generated samples\n",
        "    pixel_std = torch.std(generated_samples).item()\n",
        "\n",
        "    # Ensure shapes as expected\n",
        "    assert generated_samples.shape in [(samples_to_generate,1,28,28)], \"Generated samples shape mismatch\"\n",
        "\n",
        "    return model, generated_samples, recon_tensor, avg_kl_loss, mse_value, pixel_std, history\n",
        "\n",
        "# ------------------------\n",
        "# Example runner (call this)\n",
        "# ------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    # Recommended: run in Colab or machine with GPU\n",
        "    vae_model, generated_samples, reconstructed_samples, avg_kl, mse_val, pix_std, history = run_vae_experiment(\n",
        "        batch_size=128, epochs=30, latent_dim=20, lr=1e-3, samples_to_generate=10\n",
        "    )\n",
        "    print(\"Final results:\")\n",
        "    print(\"Generated samples shape:\", generated_samples.shape)\n",
        "    print(f\"Avg KL per image: {avg_kl:.4f}\")\n",
        "    print(f\"MSE (first 5 reconstructions): {mse_val:.6f}\")\n",
        "    print(f\"Pixel std of generated samples: {pix_std:.6f}\")\n",
        "\n",
        "    # Save some sample grids if you like:\n",
        "    utils.save_image(generated_samples, \"generated_samples.png\", nrow=5)\n",
        "    # Save original vs recon (side by side)\n",
        "    side_by_side = torch.cat([reconstructed_samples, reconstructed_samples], dim=0) # placeholder\n",
        "    utils.save_image(reconstructed_samples, \"reconstructions.png\", nrow=5)\n"
      ],
      "metadata": {
        "id": "kL8w_Uli1QQl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Save trained model ---\n",
        "model_path = \"vae_model.pth\"\n",
        "torch.save(vae_model.state_dict(), model_path)\n",
        "print(\"Model saved successfully as\", model_path)\n"
      ],
      "metadata": {
        "id": "30VPu9ixHECf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Load model ---\n",
        "loaded_model = VAEModel(latent_dim=20)\n",
        "loaded_model.load_state_dict(torch.load(\"vae_model.pth\", map_location=device))\n",
        "loaded_model.to(device)\n",
        "loaded_model.eval()\n",
        "print(\"Model loaded successfully!\")\n"
      ],
      "metadata": {
        "id": "vzvIOAZTNcLS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "XvkItceEN8vu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BQNTkBNGQfwt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}